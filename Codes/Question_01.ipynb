{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Dataloading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 50\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# 2. Define Network Parameters\n",
    "Din = 3 * 32 * 32  # Input size (flattened CIFAR-10 image size)\n",
    "H = 100  # Hidden layer size\n",
    "K = 10  # Output size (number of classes in CIFAR-10)\n",
    "std = 1e-5\n",
    "\n",
    "# Initialize weights and biases\n",
    "w1 = torch.randn(Din, H) * std  # Input to hidden layer\n",
    "b1 = torch.zeros(H)\n",
    "w2 = torch.randn(H, K) * std  # Hidden to output layer\n",
    "b2 = torch.zeros(K)\n",
    "\n",
    "# Hyperparameters\n",
    "iterations = 10\n",
    "lr = 2e-6  # Learning rate\n",
    "lr_decay = 0.9  # Learning rate decay\n",
    "reg = 0  # Regularization\n",
    "\n",
    "loss_history = []\n",
    "\n",
    "# Define Cross-Entropy Loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 3. Training Loop\n",
    "for epoch in range(iterations):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # Get inputs and labels\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]  # Batch size\n",
    "        x_train = inputs.view(Ntr, -1)  # Flatten input to (Ntr, Din)\n",
    "        \n",
    "        # Forward pass\n",
    "        h = torch.sigmoid(x_train.mm(w1) + b1)  # Sigmoid activation for the hidden layer\n",
    "        y_pred = h.mm(w2) + b2  # Output layer activation\n",
    "\n",
    "        # Loss calculation (Cross-Entropy Loss with regularization)\n",
    "        loss = criterion(y_pred, labels) + reg * (torch.sum(w1 ** 2) + torch.sum(w2 ** 2))\n",
    "        loss_history.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Backpropagation (Manual)\n",
    "        dy_pred = torch.softmax(y_pred, dim=1) - nn.functional.one_hot(labels, K).float()  # Gradient for cross-entropy\n",
    "        dw2 = h.t().mm(dy_pred) + reg * w2  # Gradient for w2\n",
    "        db2 = dy_pred.sum(dim=0)  # Gradient for b2\n",
    "\n",
    "        dh = dy_pred.mm(w2.t()) * h * (1 - h)  # Gradient through sigmoid activation\n",
    "        dw1 = x_train.t().mm(dh) + reg * w1  # Gradient for w1\n",
    "        db1 = dh.sum(dim=0)  # Gradient for b1\n",
    "\n",
    "        # Update weights and biases\n",
    "        w1 -= lr * dw1\n",
    "        b1 -= lr * db1\n",
    "        w2 -= lr * dw2\n",
    "        b2 -= lr * db2\n",
    "\n",
    "    # Print loss for every epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{iterations}], Loss: {running_loss / len(trainloader):.4f}\")\n",
    "\n",
    "\n",
    "    # Learning rate decay\n",
    "    lr *= lr_decay\n",
    "\n",
    "# 4. Plotting the Loss History\n",
    "plt.plot(loss_history)\n",
    "plt.title(\"Loss History\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()  \n",
    "\n",
    "# 5. Evaluate on Training Set\n",
    "with torch.no_grad():\n",
    "    # Training accuracy\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        Ntr = inputs.shape[0]\n",
    "        x_train = inputs.view(Ntr, -1)\n",
    "        y_train_onehot = nn.functional.one_hot(labels, K).float()\n",
    "        h = torch.sigmoid(x_train.mm(w1) + b1)\n",
    "        y_train_pred = h.mm(w2) + b2\n",
    "        predicted_train = torch.argmax(y_train_pred, dim=1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels).sum().item()\n",
    "    train_acc = 100 * correct_train / total_train\n",
    "    print(f\"Training accuracy: {train_acc:.2f}%\")\n",
    "\n",
    "# 6. Evaluate on Test Set\n",
    "with torch.no_grad():\n",
    "    # Test accuracy\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    for data in testloader:\n",
    "        inputs, labels = data\n",
    "        Nte = inputs.shape[0]\n",
    "        x_test = inputs.view(Nte, -1)\n",
    "        y_test_onehot = nn.functional.one_hot(labels, K).float()\n",
    "        h = torch.sigmoid(x_test.mm(w1) + b1)\n",
    "        y_test_pred = h.mm(w2) + b2\n",
    "        predicted_test = torch.argmax(y_test_pred, dim=1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted_test == labels).sum().item()\n",
    "    test_acc = 100 * correct_test / total_test\n",
    "    print(f\"Test accuracy: {test_acc:.2f}%\")\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
